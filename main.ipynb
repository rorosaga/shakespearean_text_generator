{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation 📓✏️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Before starting, define the **n-grams** to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_grams = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caesar length:  25833\n",
      "Hamlet length:  37360\n",
      "Macbeth length:  23140\n"
     ]
    }
   ],
   "source": [
    "# loading shakespeare's works\n",
    "\n",
    "import nltk\n",
    "nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "caesar = nltk.corpus.gutenberg.words('shakespeare-caesar.txt')\n",
    "print(\"Caesar length: \", len(caesar))\n",
    "hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')\n",
    "print(\"Hamlet length: \", len(hamlet))\n",
    "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')\n",
    "print(\"Macbeth length: \", len(macbeth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Lowering the case of the text and removing punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'tragedie', 'of', 'julius', 'caesar', 'by', 'william', 'shakespeare', '1599', 'actus']\n",
      "['the', 'tragedie', 'of', 'hamlet', 'by', 'william', 'shakespeare', '1599', 'actus', 'primus']\n",
      "['the', 'tragedie', 'of', 'macbeth', 'by', 'william', 'shakespeare', '1603', 'actus', 'primus']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = [word.lower() for word in text]\n",
    "    # Remove punctuation\n",
    "    text = [word for word in text if word not in string.punctuation]\n",
    "    return text\n",
    "\n",
    "# Preprocess texts\n",
    "caesar_clean = preprocess_text(caesar)\n",
    "hamlet_clean = preprocess_text(hamlet)\n",
    "macbeth_clean = preprocess_text(macbeth)\n",
    "\n",
    "print(caesar_clean[:10])\n",
    "print(hamlet_clean[:10])\n",
    "print(macbeth_clean[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Creating the **list of n-grams** for each text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2-grams from Caesar:\n",
      "[('the', 'tragedie'), ('tragedie', 'of'), ('of', 'julius'), ('julius', 'caesar'), ('caesar', 'by')]\n",
      "\n",
      "2-grams from Hamlet:\n",
      "[('the', 'tragedie'), ('tragedie', 'of'), ('of', 'hamlet'), ('hamlet', 'by'), ('by', 'william')]\n",
      "\n",
      "2-grams from Macbeth:\n",
      "[('the', 'tragedie'), ('tragedie', 'of'), ('of', 'macbeth'), ('macbeth', 'by'), ('by', 'william')]\n"
     ]
    }
   ],
   "source": [
    "# list of n-grams for each text\n",
    "def get_ngrams(tokens, n):\n",
    "    return list(nltk.ngrams(tokens, n))\n",
    "\n",
    "# Get n-grams for each text\n",
    "caesar_ngrams = get_ngrams(caesar_clean, n_grams)\n",
    "hamlet_ngrams = get_ngrams(hamlet_clean, n_grams)\n",
    "macbeth_ngrams = get_ngrams(macbeth_clean, n_grams)\n",
    "\n",
    "print(f\"\\n{n_grams}-grams from Caesar:\")\n",
    "print(caesar_ngrams[:5])\n",
    "print(f\"\\n{n_grams}-grams from Hamlet:\")\n",
    "print(hamlet_ngrams[:5])\n",
    "print(f\"\\n{n_grams}-grams from Macbeth:\")\n",
    "print(macbeth_ngrams[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Counting the **frequency** of each ngram's subsequent token and the own ngram's frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram 'the tragedie':\n",
      "Total count: 6\n",
      "Next tokens: {'of': 6}\n",
      "\n",
      "Raw data:\n",
      "{'count': 6, 'next_tokens': {'of': 6}}\n"
     ]
    }
   ],
   "source": [
    "# Count the frequency of each n-gram's subsequent token\n",
    "# Example: from_ngram_to_next_token_counts[('to', 'be')] = {'count': 15, 'next_tokens': {'or': 10, 'not': 5}}\n",
    "# This means that the n-gram 'to be' is followed by 'or' 10 times and by 'not' 5 times, and the total count of ngram 'to be' is 15\n",
    "\n",
    "\n",
    "from_ngram_to_next_token_counts = {}\n",
    "\n",
    "def count_next_token(ngrams, tokens, dictionary):\n",
    "    for i in range(len(ngrams)):\n",
    "        if i < len(tokens) - 2:  # Make sure we have a next token\n",
    "            ngram = ngrams[i]\n",
    "            next_token = tokens[i + 2]  # Get the token that follows the ngram\n",
    "            \n",
    "            # If ngram doesn't exist, create new entry with count and next_tokens dictionary\n",
    "            if ngram not in dictionary:\n",
    "                dictionary[ngram] = {\n",
    "                    'count': 0,\n",
    "                    'next_tokens': {}\n",
    "                }\n",
    "            \n",
    "            # Increment total count for this ngram\n",
    "            dictionary[ngram]['count'] += 1\n",
    "            \n",
    "            # Add or increment next token count\n",
    "            if next_token not in dictionary[ngram]['next_tokens']:\n",
    "                dictionary[ngram]['next_tokens'][next_token] = 1\n",
    "            else:\n",
    "                dictionary[ngram]['next_tokens'][next_token] += 1\n",
    "\n",
    "# Count next tokens from each play\n",
    "count_next_token(caesar_ngrams, caesar_clean, from_ngram_to_next_token_counts)\n",
    "count_next_token(hamlet_ngrams, hamlet_clean, from_ngram_to_next_token_counts)\n",
    "count_next_token(macbeth_ngrams, macbeth_clean, from_ngram_to_next_token_counts)\n",
    "\n",
    "\n",
    "ngram = ('the', 'tragedie')\n",
    "result = from_ngram_to_next_token_counts[ngram]\n",
    "print(f\"N-gram '{ngram[0]} {ngram[1]}':\")\n",
    "print(f\"Total count: {result['count']}\")\n",
    "print(f\"Next tokens: {result['next_tokens']}\")\n",
    "\n",
    "print(\"\\nRaw data:\")\n",
    "print(from_ngram_to_next_token_counts[('the', 'tragedie')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distribution 📊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Calculating the **probability** of each n-gram's subsequent token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for ngram 'the tragedie':\n",
      "{'of': 1.0}\n",
      "\n",
      "More examples:\n",
      "\n",
      "ngram 'the tragedie':\n",
      "Probabilities: {'of': 1.0}\n",
      "\n",
      "ngram 'tragedie of':\n",
      "Probabilities: {'julius': 0.167, 'ivlivs': 0.167, 'hamlet': 0.333, 'macbeth': 0.333}\n",
      "\n",
      "ngram 'of julius':\n",
      "Probabilities: {'caesar': 1.0}\n",
      "\n",
      "ngram 'julius caesar':\n",
      "Probabilities: {'by': 1.0}\n",
      "\n",
      "ngram 'caesar by':\n",
      "Probabilities: {'william': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Calculating the probability of each ngram's subsequent token\n",
    "\n",
    "from_ngram_to_next_token_probs = {}\n",
    "\n",
    "def calculate_probabilities(counts_dict):\n",
    "\n",
    "    probs_dict = {}\n",
    "    \n",
    "    # For each ngram in the counts dictionary\n",
    "    for ngram, data in counts_dict.items():\n",
    "        total_count = data['count']\n",
    "        next_tokens = data['next_tokens']\n",
    "        \n",
    "        # Calculate probability for each next token\n",
    "        probs = {}\n",
    "        for token, count in next_tokens.items():\n",
    "            prob = count / total_count\n",
    "            probs[token] = round(prob, 3)  # Round to 3 decimal places\n",
    "            \n",
    "        probs_dict[ngram] = probs\n",
    "    \n",
    "    return probs_dict\n",
    "\n",
    "# Calculate probabilities\n",
    "from_ngram_to_next_token_probs = calculate_probabilities(from_ngram_to_next_token_counts)\n",
    "\n",
    "# Example probabilities for the same ngram we checked before\n",
    "ngram = ('the', 'tragedie')\n",
    "print(f\"Probabilities for ngram '{ngram[0]} {ngram[1]}':\")\n",
    "print(from_ngram_to_next_token_probs[ngram])\n",
    "\n",
    "print(\"\\nMore examples:\")\n",
    "for ngram, probs in list(from_ngram_to_next_token_probs.items())[:5]:\n",
    "    print(f\"\\nngram '{ngram[0]} {ngram[1]}':\")\n",
    "    print(f\"Probabilities: {probs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Next Token 🎲\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sampling the next token **based on the probability distribution** for a given ngram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sampling for ngram 'tragedie of'\n",
      "Probability distribution: {'julius': 0.167, 'ivlivs': 0.167, 'hamlet': 0.333, 'macbeth': 0.333}\n",
      "\n",
      "Empirical distribution after 1000 samples:\n",
      "'hamlet': 0.340\n",
      "'julius': 0.186\n",
      "'ivlivs': 0.162\n",
      "'macbeth': 0.312\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sample_next_token(ngram, prob_dict):\n",
    "    \"\"\"\n",
    "    Sample the next token based on the probability distribution for a given ngram.\n",
    "    \n",
    "    Args:\n",
    "        ngram (tuple): A tuple of tokens (previous tokens)\n",
    "        prob_dict (dict): Dictionary containing probability distributions for ngrams\n",
    "    \n",
    "    Returns:\n",
    "        str: The sampled next token\n",
    "        If ngram not found, returns None\n",
    "    \"\"\"\n",
    "    # Check if ngram exists in our probability dictionary\n",
    "    if ngram not in prob_dict:\n",
    "        return None\n",
    "    \n",
    "    # Get probability distribution for this ngram\n",
    "    next_token_probs = prob_dict[ngram]\n",
    "    \n",
    "    # Get tokens and their probabilities as separate lists\n",
    "    tokens = list(next_token_probs.keys())\n",
    "    probs = list(next_token_probs.values())\n",
    "    \n",
    "    # Normalize probabilities to ensure they sum to 1\n",
    "    probs_sum = sum(probs)\n",
    "    if probs_sum > 0:\n",
    "        probs = [p/probs_sum for p in probs]\n",
    "    \n",
    "    # Sample one token based on the probability distribution\n",
    "    next_token = np.random.choice(tokens, p=probs)\n",
    "    \n",
    "    return next_token\n",
    "\n",
    "# Test the sampling function\n",
    "test_ngram = ('tragedie', 'of')\n",
    "print(f\"Testing sampling for ngram '{test_ngram[0]} {test_ngram[1]}'\")\n",
    "print(f\"Probability distribution: {from_ngram_to_next_token_probs[test_ngram]}\")\n",
    "\n",
    "# Sample multiple times to see the distribution\n",
    "n_samples = 1000\n",
    "samples = [sample_next_token(test_ngram, from_ngram_to_next_token_probs) for _ in range(n_samples)]\n",
    "\n",
    "# Calculate and print the empirical distribution\n",
    "unique_tokens = set(samples)\n",
    "empirical_dist = {token: samples.count(token)/n_samples for token in unique_tokens}\n",
    "\n",
    "print(f\"\\nEmpirical distribution after {n_samples} samples:\")\n",
    "for token, prob in empirical_dist.items():\n",
    "    print(f\"'{token}': {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text 📝\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generating text starting from an n-gram and a specified amount of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text Examples:\n",
      "\n",
      "Starting with 'the tragedie':\n",
      "the tragedie of macbeth marry he was likely had he his hurts before rosse i and truly you were not\n",
      "\n",
      "Starting with 'to be':\n",
      "to be sounded more then all the gods to day decius neuer feare that if againe this apparition come he\n",
      "\n",
      "Starting with 'william shakespeare':\n",
      "william shakespeare 1603 actus primus scoena prima thunder and lightning enter caska and those that should be prickt to dye\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text_from_ngram(initial_ngram, num_words, prob_dict):\n",
    "    \"\"\"\n",
    "    Generate text starting from an initial n-gram.\n",
    "    \n",
    "    Args:\n",
    "        initial_ngram (tuple): The starting n-gram\n",
    "        num_words (int): Number of words to generate\n",
    "        prob_dict (dict): Dictionary containing probability distributions\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated text\n",
    "    \"\"\"\n",
    "    # Initialize text with the initial n-gram\n",
    "    generated_words = list(initial_ngram)\n",
    "    \n",
    "    # Generate remaining words\n",
    "    current_ngram = initial_ngram\n",
    "    for _ in range(num_words - len(initial_ngram)):\n",
    "        # Sample next token\n",
    "        next_token = sample_next_token(current_ngram, prob_dict)\n",
    "        \n",
    "        # If we can't continue (no following tokens found), break\n",
    "        if next_token is None:\n",
    "            break\n",
    "            \n",
    "        # Add the new token to our generated text\n",
    "        generated_words.append(next_token)\n",
    "        \n",
    "        # Create new n-gram for next iteration\n",
    "        current_ngram = tuple(generated_words[-n_grams:])\n",
    "    \n",
    "    # Join all words with spaces\n",
    "    return ' '.join(generated_words)\n",
    "\n",
    "# Test the text generation with different initial n-grams\n",
    "test_cases = [\n",
    "    ('the', 'tragedie'),\n",
    "    ('to', 'be'),\n",
    "    ('william', 'shakespeare')\n",
    "]\n",
    "\n",
    "print(\"Generated Text Examples:\\n\")\n",
    "for initial_ngram in test_cases:\n",
    "    print(f\"Starting with '{initial_ngram[0]} {initial_ngram[1]}':\")\n",
    "    generated_text = generate_text_from_ngram(initial_ngram, 20, from_ngram_to_next_token_probs)\n",
    "    print(f\"{generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results 🏁\n",
    "\n",
    "## **2-grams**\n",
    "\n",
    "Generated text examples with 20 words:\n",
    "\n",
    "Starting with 'the tragedie':\n",
    ">- the tragedie of macbeth marry he was likely had he his hurts before rosse i and truly you were not\n",
    "\n",
    "Starting with 'to be':\n",
    ">- to be sounded more then all the gods to day decius neuer feare that if againe this apparition come he\n",
    "\n",
    "Starting with 'william shakespeare':\n",
    ">- william shakespeare 1603 actus primus scoena prima thunder and lightning enter caska and those that should be prickt to dye\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
